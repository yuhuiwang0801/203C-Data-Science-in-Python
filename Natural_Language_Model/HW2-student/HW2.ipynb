{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 2: Markov Models of Natural Language\n",
    "\n",
    "### Name: Yuhui Wang\n",
    "### Collaborators: [Your collaborators]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework focuses on topics related to string manipulation, dictionaries, and simulations. \n",
    "\n",
    "I encourage collaborating with your peers, but the final text, code, and comments in this homework assignment should still be written by you. Please check the collaboration policy.\n",
    "\n",
    "Submission instructions: \n",
    "- Submit `HW2.py` and `HW2.ipynb` compressed in a one zip file on Gradescope under \"HW2 - Autograder\". Do **NOT** change the file name. \n",
    "- Convert this notebook into a pdf file and submit it on GradeScope under \"HW2 - PDF\". Make sure your text outputs in the latter problems are visible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models\n",
    "\n",
    "Many of you may have encountered the output of machine learning models which, when \"seeded\" with a small amount of text, produce a larger corpus of text which is expected to be similar or relevant to the seed text. For example, there's been a lot of buzz about the new [GPT-3 model](https://en.wikipedia.org/wiki/GPT-3), related to its [carbon footprint](https://www.forbes.com/sites/robtoews/2020/06/17/deep-learnings-climate-change-problem/#2781c1b16b43), [bigoted tendencies](https://medium.com/fair-bytes/how-biased-is-gpt-3-5b2b91f1177), and, yes, impressive (and often [humorous](https://aiweirdness.com/)) [ability to replicate human-like text in response to prompts.](https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/) \n",
    "\n",
    "We are not going to program a complicated deep learning model, but we will construct a much simpler language model that performs a similar task. Using tools like iteration and dictionaries, we will create a family of **Markov language models** for generating text. For the purposes of this assignment, an $n$-th order Markov model is a function that constructs a string of text one letter at a time, using only knowledge of the most recent $n$ letters. You can think of it as a writer with a \"memory\" of $n$ letters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports your functions defined in HW2.py \n",
    "\n",
    "from HW2 import count_characters, count_ngrams, markov_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Our training text for this exercise comes from Jane Austen's novel *Emma*, which Professor Chodrow retrieved from the archives at ([Project Gutenberg](https://www.gutenberg.org/files/158/158-h/158-h.htm#link2H_4_0001)). Intuitively, we are going to write a program that \"writes like Jane Austen,\" albeit in a very limited sense. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emma-full.txt', 'r') as f:\n",
    "    s = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1: Define `count_characters` in HW2.py\n",
    "\n",
    "Write a function called `count_characters` that counts the number of times each character appears in a user-supplied string `s`. Your function should loop over each element of the string, and sequentually update a `dict` whose keys are characters and whose values are the number of occurrences seen so far.  Your function should then return this dictionary. \n",
    "\n",
    "You may know of other ways to achieve the same result. However, you are encouraged to use the loop approach, since this will generalize to the next exercise.\n",
    "\n",
    "*Note: While the construct `for character in s:` will work for this exercise, it will not generalize to the next one. Consider using `for i in range(len(s)):` instead.* \n",
    "\n",
    "### Example usage: \n",
    "\n",
    "```python\n",
    "count_characters(\"Torto ise!\")\n",
    "{'T': 1, 't' : 1, 'o' : 2, 'r' : 1, 'i' : 1, 's' : 1, 'e' : 1, ' ': 1, '!': 1}\n",
    "```\n",
    "\n",
    "***Hint***: Yes, you did a problem very similar to this one on HW1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_characters(s):\n",
    "    d = {}\n",
    "    for i in range(len(s)):\n",
    "        if s[i] not in d:\n",
    "            d[s[i]] = 1\n",
    "        else:\n",
    "            d[s[i]] += 1\n",
    "            \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'T': 1, 'o': 2, 'r': 1, 't': 1, ' ': 1, 'i': 1, 's': 1, 'e': 1, '!': 1}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your count_characters here\n",
    "count_characters(\"Torto ise!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many times does 't' appear in Emma? How about '!'?\n",
    "\n",
    "How many different types of characters are in this dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'t' appears for 58067 times\n",
      "'!' appears for 1063 times\n",
      "There are 82 different types of characters\n"
     ]
    }
   ],
   "source": [
    "# write your answers here\n",
    "dict_emma = count_characters(s)\n",
    "print(\"'t' appears for \" + str(dict_emma['t']) + \" times\")\n",
    "print(\"'!' appears for \" + str(dict_emma['!']) + \" times\")\n",
    "\n",
    "print(\"There are \" + str(len(dict_emma)) + \" different types of characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Define `count_ngrams` in HW2.py\n",
    "\n",
    "An `n`-*gram* is a sequence of `n` letters. For example, `bol` and `old` are the two 3-grams that occur in the string `bold`.\n",
    "\n",
    "Write a function called `count_ngrams` that counts the number of times each `n`-gram occurs in a string, with `n` specified by the user and with default value `n = 1`. Your function should return the dictionary. You should be able to do this by making only a small modification to `count_characters`. \n",
    "\n",
    "### Example usage: \n",
    "\n",
    "```python\n",
    "count_ngrams(\"tortoise\", n = 2)\n",
    "```\n",
    "```\n",
    "{'to': 2, 'or': 1, 'rt': 1, 'oi': 1, 'is': 1, 'se': 1} # output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ngrams(string, n = 1):\n",
    "    counts = {}\n",
    "    for i in range(len(string) - n + 1):\n",
    "        ngram = string[i:i+n]\n",
    "        if ngram in counts:\n",
    "            counts[ngram] += 1\n",
    "        else:\n",
    "            counts[ngram] = 1\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 2, 'or': 1, 'rt': 1, 'oi': 1, 'is': 1, 'se': 1}\n",
      "{'t': 2, 'o': 2, 'r': 1, 'i': 1, 's': 1, 'e': 1}\n"
     ]
    }
   ],
   "source": [
    "# test your count_ngrams here\n",
    "print(count_ngrams(\"tortoise\", n = 2))\n",
    "print(count_ngrams(\"tortoise\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many different types of 2-grams are in this dictionary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1236 different types of 2-grams\n"
     ]
    }
   ],
   "source": [
    "# write your answer here\n",
    "dic = count_ngrams(s, n = 2)\n",
    "print(\"There are \" + str(len(dic)) + \" different types of 2-grams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 3: Define `markov_text` in HW2.py\n",
    "\n",
    "Now we are going to use our `n`-grams to generate some fake text according to a Markov model. Here's how the Markov model of order `n` works: \n",
    "\n",
    "### A. Compute (`n`+1)-gram occurrence frequencies\n",
    "\n",
    "You have already done this in Problem 2!  \n",
    "\n",
    "### B. Starting `n`-gram\n",
    "\n",
    "The starting `n`-gram is the last `n` characters in the argument `seed`.\n",
    "\n",
    "### C. Generate Text\n",
    "\n",
    "Now we generate text one character at a time. To do so:\n",
    "\n",
    "1. Look at the most recent `n` characters in our generated text. Say that `n = 3` and the 3 most recent character are `the`. \n",
    "2. We then look at our list of `n+1`-grams, and focus on grams whose first `n` characters match. Examples matching `the` include `them`, `the `, `thei`, and so on. \n",
    "3. We pick a random one of these `n+1`-grams, weighted according to its number of occurrences. \n",
    "4. The final character of this new `n+1` gram is our next letter. \n",
    "\n",
    "For example, if there are 3 occurrences of `them`, 4 occurrences of `the `, and 1 occurrences of `thei` in the n-gram dictionary, then our next character is `m` with probabiliy 3/8, `[space]` with probability 1/2, and `i` with probability `1/8`. \n",
    "\n",
    "**Remember**: the ***3rd***-order model requires you to compute ***4***-grams. \n",
    "\n",
    "## What you should do\n",
    "\n",
    "Write a function `markov_text` that generates synthetic text according to an `n`-th order Markov model. It should have the following arguments: \n",
    "\n",
    "- `s`, the input string of real text. \n",
    "- `n`, the order of the model. \n",
    "- `length`, the size of the text to generate. Use a default value of 100. \n",
    "-  `seed`, the initial string that gets the Markov model started. I used `\"Emma Woodhouse\"` (the full name of the protagonist of the novel) as my `seed`, but any subset of `s` of length `n` or larger will work. \n",
    "\n",
    "It should return a string with the length of `len(seed) + length`.\n",
    "\n",
    "Demonstrate the output of your function for a couple different choices of the order `n`. \n",
    "\n",
    "\n",
    "## Expected Output\n",
    "\n",
    "Here are a few examples of the output of this function. Because of randomness, your results won't look exactly like this, but they should be qualitatively similar. \n",
    "\n",
    "```python\n",
    "markov_text(s, n = 2, length = 200, seed = \"Emma Woodhouse\")\n",
    "```\n",
    "```\n",
    "Emma Woodhouse ne goo thimser. John mile sawas amintrought will on I kink you kno but every sh inat he fing as sat buty aft from the it. She cousency ined, yount; ate nambery quirld diall yethery, yould hat earatte\n",
    "```\n",
    "```python\n",
    "markov_text(s, n = 4, length = 200, seed = \"Emma Woodhouse\")\n",
    "```\n",
    "\n",
    "```\n",
    "Emma Woodhouse!”—Emma, as love,            Kitty, only this person no infering ever, while, and tried very were no do be very friendly and into aid,    Man's me to loudness of Harriet's. Harriet belonger opinion an\n",
    "```\n",
    "\n",
    "```python\n",
    "markov_text(s, n = 10, length = 200, seed = \"Emma Woodhouse\")\n",
    "```\n",
    "\n",
    "```\n",
    "Emma Woodhouse's party could be acceptable to them, that if she ever were disposed to think of nothing but good. It will be an excellent charade remains, fit for any acquainted with the child was given up to them.\n",
    "```\n",
    "\n",
    "## Notes and Hints\n",
    "\n",
    "***Hint***: A good function for performing the random choice is the `choices()` function in the `random` module. You can use it like this: \n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "options = [\"One\", \"Two\", \"Three\"]\n",
    "weights = [1, 2, 3] # \"Two\" is twice as likely as \"One\", \"Three\" three times as likely. \n",
    "\n",
    "random.choices(options, weights) \n",
    "```\n",
    "\n",
    "```\n",
    "['One'] # output\n",
    "```\n",
    "\n",
    "The first and second arguments must be lists of equal length. Note also that the return value is a list -- if you want the value *in* the list, you need to get it out via indexing.  \n",
    "\n",
    "***Note***: For grading purposes, the `options` should be the possible `n+1`-grams in the order of first appeareance in the text. If you are working through the strings from beginning to end, you will not have issues with this, as dictionary keys are ordered. Please do NOT use [`random.seed()`](https://www.w3schools.com/python/ref_random_seed.asp) in your function -- the autograder code will do it. You are welcome to try it out in your notebook for reproducible results if you are interested. \n",
    "\n",
    "***Hint***: The first thing your function should do is call `count_ngrams` above to generate the required dictionary. Then, handle the logic described above in the main loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def markov_text(s, n, length=100, seed=\"\"):\n",
    "    ngram_counts = count_ngrams(s, n+1)\n",
    "    generated_text = seed\n",
    "\n",
    "    while len(generated_text) < len(seed) + length:\n",
    "        current_ngram = generated_text[-n:]\n",
    "        possible_next_chars = [key[n] for key in ngram_counts if key.startswith(current_ngram)]\n",
    "        weights = [ngram_counts[current_ngram + char] for char in possible_next_chars]\n",
    "        next_char = random.choices(possible_next_chars, weights)[0]\n",
    "        generated_text += next_char\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emma Woodhouse, youbt.\\n\\n“Oh! He a re ut whould yousinevely crity.”\\n\\n“I hichis comperhany id, becte exces.\\n\\n“But saine practuad the not ithery sup, spery to eve he ind I died no hopleres sompayse, I hown, soo yourch'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test your markov_text here\n",
    "import random\n",
    "markov_text(s, n = 2, length = 200, seed = \"Emma Woodhouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 4\n",
    "\n",
    "Using a `for`-loop, print the output of your function for `n` ranging from `1` to `10` (including 10).\n",
    "\n",
    "Then, write down a few observations. How does the generated text depend on `n`? How does the time required to generate the text depend on `n`? Do your best to explain each observation.  \n",
    "\n",
    "What do you think could happen if you were to repeat this assignment but in unit of words and not in unit of characters? For example, 2-grams would indicate two words, and not two characters.\n",
    "\n",
    "What heuristics would you consider adding to your model to improve its prediction performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your observations and thoughts here\n",
    "\n",
    "# From the results, we can see that the generated texts become more coherent and \n",
    "# contextually relevant when n increases. \n",
    "# That is because the higher order model will consider a longer history characters. \n",
    "# When n is small, the generated text will appear more random and less meaningful.\n",
    "\n",
    "# The time required will increase along with the \n",
    "# increase of n, as a higher order model involves more computations.\n",
    "\n",
    "# Using word-level n-grams will make the generated texts more \n",
    "# coherent and meaningful, as it will capture higher-level \n",
    "# semantics and syntactic structures compared to character-level n-grams.\n",
    "\n",
    "# Feature engineering, for example, adding contextual \n",
    "# features and linguistic knowledge, will help the model\n",
    "# understand the previous text and predict more accurately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Markov text with n=1:\n",
      "Emma Woodhousey. ay m dldhas try. uca ie mind rofare ps s ct ped ag w Mrerat mesuanca bldevot hichurerry, sereathat or ckercie acabuther unne nge weas. aithte; ptor aspte y Kndoforint vereraies mpyotsit heving he t\n",
      "\n",
      "\n",
      "Markov text with n=2:\n",
      "Emma Woodhouse.—She Emma con diest. Youly purried ges, to in a wholly lon ard te te couressurse, he thea-bat ito Mrselto John, sen motheir a whand equiettle.—But ateng a caund friderapeat have shour ton. She hille \n",
      "\n",
      "\n",
      "Markov text with n=3:\n",
      "Emma Woodhouse a be unce, rapture I me occasisteptabliged when, and as states herward taledge as in angere. If Mr. West put outs, (glad by her. Mrs. Westing to him, my desenseeing to been inted I man at not pres—cu\n",
      "\n",
      "\n",
      "Markov text with n=4:\n",
      "Emma Woodhouse made.\n",
      "\n",
      "The every smoothink outcry. After have carry on start, this have been the whened, I do the quests bears, the subjects interest, then Mrs. Cole of paid, a regular come and of it! I say, by read\n",
      "\n",
      "\n",
      "Markov text with n=5:\n",
      "Emma Woodhouse, and call on her in general apples and might hold me if Jane; and she long on freak so sorry, honest repetitionable; and, and coarse not see young to do. That, but it will not much you should only yo\n",
      "\n",
      "\n",
      "Markov text with n=6:\n",
      "Emma Woodhouse, would I ever parent, if I could not find a pretty letter if it were he made to herself—and a very well enough to praise of his own will done this apologies; and stood out of Harriet; and to get inti\n",
      "\n",
      "\n",
      "Markov text with n=7:\n",
      "Emma Woodhouse. I merely employed in tranquillised and construction for the subject on succeeded in the arrangement made her at once.—Something beyond what we never shall. And Mr. John Knightley. I will not liking \n",
      "\n",
      "\n",
      "Markov text with n=8:\n",
      "Emma Woodhouse had not be consent before, are unjust to Hartfield, perhaps, or a little just—every imagination. Harriet was puzzling over-trimmed; I have no doubt; accomplished; and every thing he was quite alone. \n",
      "\n",
      "\n",
      "Markov text with n=9:\n",
      "Emma Woodhouse! who can say how perfectly remembrance.”\n",
      "\n",
      "“I am,”—she answer, but we shall gradually done full justice—only we do now, it is clear; the state, I assure you, Mr. Knightley, what a perfectly dry. Come \n",
      "\n",
      "\n",
      "Markov text with n=10:\n",
      "Emma Woodhouse and Emma, at last, when Mrs. Weston kindly and persuaded that you would be wanted, and his father’s being gone out to put the horse were useable; but he was sufferings acted as a cure of every thing \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# run your markov_text here\n",
    "for n in range(1, 11):\n",
    "    print(f\"Markov text with n={n}:\")\n",
    "    print(markov_text(s, n=n, length=200, seed=\"Emma Woodhouse\"))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 5\n",
    "\n",
    "Try running your program with a different text! \n",
    "\n",
    "You can \n",
    "- find any movie script from https://imsdb.com/ or a book by Shakespeare, Hemingway, Beowulf, O.Henry, A.A. Milne, etc from https://www.gutenberg.org/\n",
    "- ctrl + a to select all text on the page\n",
    "- copy paste into a new `.txt` file. let's call it `book.txt`.\n",
    "- put `book.txt` in the same folder as emma-full.txt.\n",
    "- run the following code to read the file into variable `s`.\n",
    "```python\n",
    "with open('book.txt', 'r') as f:\n",
    "    s = f.read()\n",
    "```\n",
    "- run `markov_text` on `s` with appropriate parameters.\n",
    "\n",
    "Show your output here. Which parameters did you pick and why? Do you see any difference from when you ran the program with Emma? How so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('book.txt', 'r') as f:\n",
    "    s = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nerve enough\\nThis ebook is for the use of anyone anywhere in the United States with eBooks not protected by U.S. copyright law. Redistributing, performing, displayed their tiny square outlines in regular patterns around the ground. There was no longer any rush of wind or roar of motor; nothing but a few tatters of silk and several shroud lines were securely held by the rudder.\\nO’Connell’s eyes glinted.\\n“’Tis not a bad idea at all,” he admitted, and looked at each other. Determination was imprinted in the lines of both countenances, and together they squirmed to their feet in that cramped compartment.\\nThe motor labored on, and both men thrust feet out straight, and moved shoulders tentatively, as if to drive away any incipient stiffness that might hinder action in that one swift leap into space.\\nBut ju'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run your new code\n",
    "markov_text(s, 12, 800, seed=\"Nerve enough\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I choose n = 12 length = 800, and seed = \"Nerve enough\". \n",
    "# This is because \"Nerve enough\" is the book name, \n",
    "# and n = 12 is the maximum length of the book name. \n",
    "# Choosing the max n can help the model understand the text best.\n",
    "\n",
    "# This generated text followed the original text more, \n",
    "# as I set the parameter higher to involve more computation \n",
    "# and learn more from previous text."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
